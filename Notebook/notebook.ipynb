{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8fef2a-741d-4977-b8b3-5337f7bc0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe18027-9e3b-4628-865c-1417a05b8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06b55abf-3aa0-429d-8c64-842dbdcf3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Training data, Test data\n",
    "#This code loads datasets into Pandas DataFrames to prepare for data processing, model training, and predictions. The datasets are in CSV (Comma-Separated Values) format, which is a standard format for structured data.\n",
    "training_df = pd.read_csv(\"train_land_cover_assignment.csv\")\n",
    "test_df = pd.read_csv(\"test_land_cover_assignment.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86e1a146-88bc-4c33-94c6-1918ff094cc6",
   "metadata": {},
   "source": [
    "Visualizing nature of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fb0e679-748a-4ec5-a205-eb38f3b4287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subid', 'lat', 'lon', 'building', 'cropland', 'wcover', 'bcount', 'x',\n",
      "       'y', 'bd20', 'bio1', 'bio12', 'bio7', 'bio15', 'cec20', 'dipa', 'dni',\n",
      "       'dnlt', 'dnpa', 'dor1', 'dor2', 'fpara', 'fpars', 'lcc10', 'lcc11',\n",
      "       'lcc12', 'lcc13', 'lcc14', 'lcc21', 'lcc8', 'lcc9', 'lstd', 'lstn',\n",
      "       'mb1', 'mb2', 'mb3', 'mb7', 'mdem', 'mlat', 'mlon', 'nppm', 'npps',\n",
      "       'ph20', 'sirm', 'sirs', 'slope', 'snd20', 'soc20', 'tim'],\n",
      "      dtype='object')\n",
      "Index(['subid', 'lat', 'lon', 'bcount', 'x', 'y', 'bd20', 'bio1', 'bio12',\n",
      "       'bio7', 'bio15', 'cec20', 'dipa', 'dni', 'dnlt', 'dnpa', 'dor1', 'dor2',\n",
      "       'fpara', 'fpars', 'lcc10', 'lcc11', 'lcc12', 'lcc13', 'lcc14', 'lcc21',\n",
      "       'lcc8', 'lcc9', 'lstd', 'lstn', 'mb1', 'mb2', 'mb3', 'mb7', 'mdem',\n",
      "       'mlat', 'mlon', 'nppm', 'npps', 'ph20', 'sirm', 'sirs', 'slope',\n",
      "       'snd20', 'soc20', 'tim'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#This code prints the column names of the training and test datasets.\n",
    "#It helps in understanding the structure of the datasets before preprocessing.\n",
    "print(training_df.columns)\n",
    "print(test_df.columns)\n",
    "\n",
    "#training_df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "925a9fc1-ec53-41da-9749-7f4f889a4263",
   "metadata": {},
   "source": [
    "Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48879e10-e664-4bce-9357-2e9f08a41be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subid        0\n",
      "lat          0\n",
      "lon          0\n",
      "building     0\n",
      "cropland     0\n",
      "wcover       0\n",
      "bcount       0\n",
      "x            0\n",
      "y            0\n",
      "bd20        45\n",
      "bio1         0\n",
      "bio12        0\n",
      "bio7         0\n",
      "bio15        0\n",
      "cec20       45\n",
      "dipa         0\n",
      "dni          0\n",
      "dnlt         0\n",
      "dnpa         0\n",
      "dor1         0\n",
      "dor2         0\n",
      "fpara        0\n",
      "fpars        0\n",
      "lcc10        0\n",
      "lcc11        0\n",
      "lcc12        0\n",
      "lcc13        0\n",
      "lcc14        0\n",
      "lcc21        0\n",
      "lcc8         0\n",
      "lcc9         0\n",
      "lstd        19\n",
      "lstn        19\n",
      "mb1          1\n",
      "mb2          1\n",
      "mb3          1\n",
      "mb7          1\n",
      "mdem         0\n",
      "mlat         0\n",
      "mlon         0\n",
      "nppm         0\n",
      "npps         0\n",
      "ph20        45\n",
      "sirm         0\n",
      "sirs         0\n",
      "slope        0\n",
      "snd20       45\n",
      "soc20       45\n",
      "tim          1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#This command checks for missing (null) values in the training dataset (training_df).\n",
    "#It helps identify whether there are any empty or missing entries in the dataset,\n",
    "print(training_df.isnull().sum())#checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02d13fef-e0e6-4c82-8602-a8f3e5d840fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      building cropland wcover\n",
      "0           No       No   >60%\n",
      "1           No      Yes   <30%\n",
      "2           No      Yes   <30%\n",
      "3           No       No   <30%\n",
      "4           No       No   <30%\n",
      "...        ...      ...    ...\n",
      "15851       No       No   <30%\n",
      "15852       No       No   >60%\n",
      "15853       No       No   <30%\n",
      "15854       No       No   >60%\n",
      "15855       No      Yes   >60%\n",
      "\n",
      "[15856 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#This code splits the dataset into features (input variables) and target labels (output variables)\n",
    "#If any of the specified columns (subid, building, cropland,wcover) are missing, Python won't raise an error\n",
    "training_elements = training_df.drop(columns=['subid', 'building', 'cropland', 'wcover'], errors='ignore')  # These are target labels.\n",
    "training_labels = training_df[['building', 'cropland', 'wcover']] #Extracting target labels from the dataframe\n",
    "print (training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bee10be6-a813-48ac-881d-8d2a9f1e7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the missing values with median value\n",
    "training_elements.fillna(training_elements.median(), inplace=True)\n",
    "test_df.fillna(test_df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b8f1a3f-0591-498f-a87c-9505e1d38aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of data to ensure all numerical input features have uniform scale\n",
    "#Standardization (Normalization) ensures that all numerical features have the same scale\n",
    "scaler = StandardScaler()  # Initialization of standard scaler\n",
    "training_elements_scaled = scaler.fit_transform(training_elements)\n",
    "test_features_scaled = scaler.transform(test_df.drop(columns=['subid'], errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2c63ae9-e30d-4507-9ef9-449fbcc3fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding to convert categorical labels to numerical format to be read by model\n",
    "label_encoders = {col: LabelEncoder().fit(training_labels[col]) for col in training_labels.columns}#Creates a dictionary of encoders for each label column\n",
    "train_labels_encoded = np.column_stack([label_encoders[col].transform(training_labels[col]) for col in training_labels.columns])#Converts each categorical label into numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30aedfae-faa3-4887-bed4-1f3496340e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-validation split to divide the dataset into training and validation subsets.\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_elements_scaled, train_labels_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aa91a9f-3e39-4b88-b9ce-8a404b6017da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization and training of separate Random Forest Classifier for each land cover category\n",
    "# Each classifier is trained separatelyto predict whether a given instance belongs to that class.\n",
    "rf_models = {col: RandomForestClassifier(n_estimators=100, random_state=42) for col in training_labels.columns}\n",
    "for i, col in enumerate(training_labels.columns):\n",
    "    rf_models[col].fit(X_train, y_train[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cac99ae-ba13-48bd-add0-b01ff451e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for building:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2912\n",
      "           1       1.00      0.99      1.00       260\n",
      "\n",
      "    accuracy                           1.00      3172\n",
      "   macro avg       1.00      1.00      1.00      3172\n",
      "weighted avg       1.00      1.00      1.00      3172\n",
      "\n",
      "Classification Report for cropland:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85      2132\n",
      "           1       0.74      0.57      0.64      1040\n",
      "\n",
      "    accuracy                           0.79      3172\n",
      "   macro avg       0.78      0.74      0.75      3172\n",
      "weighted avg       0.79      0.79      0.79      3172\n",
      "\n",
      "Classification Report for wcover:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60       839\n",
      "           1       0.41      0.19      0.26       915\n",
      "           2       0.63      0.82      0.71      1418\n",
      "\n",
      "    accuracy                           0.59      3172\n",
      "   macro avg       0.54      0.54      0.53      3172\n",
      "weighted avg       0.55      0.59      0.55      3172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model validation and evaluating its perfomance\n",
    "y_val_pred = np.column_stack([rf_models[col].predict(X_val) for col in training_labels.columns])  # Predict on validation data\n",
    "for i, col in enumerate(training_labels.columns):\n",
    "    print(f\"Classification Report for {col}:\")  # Print evaluation metric\n",
    "    print(classification_report(y_val[:, i], y_val_pred[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbb484a0-d99d-4a91-90f3-4cf2240824bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions for test set:\n",
    "#predicts the probability that a test sample belongs to a given land cover category (Buildings, Cropland, Woody Vegetation Cover).\n",
    "test_predictions = {col: rf_models[col].predict_proba(test_features_scaled)[:, 1] for col in training_labels.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6816405b-d6fc-4d4f-b786-944596dbdcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file generated: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({'subid': test_df['subid']})  # create submission dataframe\n",
    "for col in training_labels.columns:\n",
    "    submission[col] = test_predictions[col]  # Add predicted probabilities\n",
    "submission.to_csv(\"submission.csv\", index=False)  # Save submission file\n",
    "\n",
    "print(\"Submission file generated: submission.csv\")  # Confirm completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430657e-001a-459e-9de0-efac081f6421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
